{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce076255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Gradient descent for logistic regression\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    W = np.zeros(n + 1)\n",
    "    X_b = np.c_[np.ones((m, 1)), X]\n",
    "    for epoch in range(epochs):\n",
    "        z = X_b.dot(W)\n",
    "        y_pred = sigmoid(z)\n",
    "        error = y_pred - y\n",
    "        gradient = (1/m) * X_b.T.dot(error)\n",
    "        W -= learning_rate * gradient\n",
    "        if epoch % 100 == 0:\n",
    "            loss = - (1/m) * np.sum(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    return W\n",
    "\n",
    "# Load Iris dataset (binary classification)\n",
    "iris = load_iris()\n",
    "X = iris.data[:100, :2]  # first 2 features\n",
    "y = (iris.target[:100] == 1).astype(int)  # binary labels\n",
    "\n",
    "# Train model\n",
    "weights = gradient_descent(X, y, learning_rate=0.1, epochs=1000)\n",
    "print(\"\\nIntercept (w0):\", weights[0])\n",
    "print(\"Weights:\", weights[1:])\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', label='Data Points')\n",
    "x_values = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "y_values = -(weights[0] + weights[1] * x_values) / weights[2]\n",
    "plt.plot(x_values, y_values, color='black', label='Decision Boundary')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Logistic Regression on Iris Dataset')\n",
    "plt.legend()\n",
    "plt.savefig('Figure.png') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26d661",
   "metadata": {},
   "source": [
    "```text\n",
    "Epoch 0, Loss: 0.6931471785599451, Gradient: [ 0.     -0.2325  0.1645]\n",
    "Epoch 100, Loss: 0.3481046452085188, Gradient: [ 0.01148113 -0.06474227  0.11038511]\n",
    "Epoch 200, Loss: 0.2418913519815763, Gradient: [ 0.00861166 -0.04171104  0.07047584]\n",
    "Epoch 300, Loss: 0.19196356374056442, Gradient: [ 0.0071644  -0.03079191  0.05166457]\n",
    "Epoch 400, Loss: 0.1628495378832644, Gradient: [ 0.00629626 -0.02449043  0.04084593]\n",
    "Epoch 500, Loss: 0.14366689360303794, Gradient: [ 0.005716   -0.02039372  0.0338301 ]\n",
    "Epoch 600, Loss: 0.1300058830359666, Gradient: [ 0.0052991  -0.0175144   0.02890893]\n",
    "Epoch 700, Loss: 0.1197411488691946, Gradient: [ 0.0049838  -0.01537717  0.02526214]\n",
    "Epoch 800, Loss: 0.11172040549501193, Gradient: [ 0.00473608 -0.01372565  0.02244812]\n",
    "Epoch 900, Loss: 0.1052634829346734, Gradient: [ 0.00453564 -0.01240951  0.02020835]\n",
    "Intercept (w0): -0.6933893945914748\n",
    "Weights: [ 3.04271251 -5.09949597]\n",
    "```\n",
    "\n",
    "![Gradient Descent Figure](Figure.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
